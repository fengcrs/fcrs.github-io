<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>目标检测 - RCNN 系列浅谈 | bruceyang</title>
    <meta name="generator" content="VuePress 1.7.1">
    <link rel="shortcut icon" href="https://avatars1.githubusercontent.com/u/33708068?s=400&amp;u=45d2437867b7706283aa9f1b7300b822e9c85f1b&amp;v=4">
    <link rel="apple-touch-icon" href="https://avatars1.githubusercontent.com/u/33708068?s=400&amp;u=45d2437867b7706283aa9f1b7300b822e9c85f1b&amp;v=4">
    <meta name="description" content="技术博客">
    <meta name="theme-color" content="#3eaf7c">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    
    <link rel="preload" href="/assets/css/0.styles.e09f4a35.css" as="style"><link rel="preload" href="/assets/js/app.32056bf2.js" as="script"><link rel="preload" href="/assets/js/2.3d662a95.js" as="script"><link rel="preload" href="/assets/js/8.54d04af0.js" as="script"><link rel="prefetch" href="/assets/js/10.f83da8c2.js"><link rel="prefetch" href="/assets/js/11.7a5d54ba.js"><link rel="prefetch" href="/assets/js/12.2880fe57.js"><link rel="prefetch" href="/assets/js/13.6a5f99b9.js"><link rel="prefetch" href="/assets/js/14.a80bca2f.js"><link rel="prefetch" href="/assets/js/3.1c9e49f8.js"><link rel="prefetch" href="/assets/js/4.25270e6b.js"><link rel="prefetch" href="/assets/js/5.0064911a.js"><link rel="prefetch" href="/assets/js/6.7d0ef18d.js"><link rel="prefetch" href="/assets/js/7.901f9289.js"><link rel="prefetch" href="/assets/js/9.ba305593.js">
    <link rel="stylesheet" href="/assets/css/0.styles.e09f4a35.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">bruceyang</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/Algorithm/" class="nav-link router-link-active">
  算法
</a></div><div class="nav-item"><a href="/Java/" class="nav-link">
  后端
</a></div><div class="nav-item"><a href="/Frontend/" class="nav-link">
  前端
</a></div><div class="nav-item"><a href="/Database/" class="nav-link">
  数据库
</a></div><div class="nav-item"><a href="/Tools/" class="nav-link">
  工具
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/Algorithm/" class="nav-link router-link-active">
  算法
</a></div><div class="nav-item"><a href="/Java/" class="nav-link">
  后端
</a></div><div class="nav-item"><a href="/Frontend/" class="nav-link">
  前端
</a></div><div class="nav-item"><a href="/Database/" class="nav-link">
  数据库
</a></div><div class="nav-item"><a href="/Tools/" class="nav-link">
  工具
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>算法</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Algorithm/" aria-current="page" class="sidebar-link">简介</a></li><li><a href="/Algorithm/chapter01-two-stage-detection.html" aria-current="page" class="active sidebar-link">目标检测 - RCNN 系列浅谈</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#fa-zhan-gui-ji" class="sidebar-link">发展轨迹</a></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#rcnn-she-ji-si-xiang" class="sidebar-link">RCNN 设计思想</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#r-cnn" class="sidebar-link">R-CNN</a></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#fast-r-cnn" class="sidebar-link">Fast R-CNN</a></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#faster-r-cnn" class="sidebar-link">Faster R-CNN</a></li></ul></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#yi-xie-gai-jin-wang-luo-de-xiao-ji-qiao" class="sidebar-link">一些改进网络的小技巧</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#fpn" class="sidebar-link">FPN</a></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#focal-loss" class="sidebar-link">Focal Loss</a></li></ul></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#yi-xie-ji-yu-rcnn-de-yan-sheng" class="sidebar-link">一些基于 RCNN 的衍生</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#r-c3d" class="sidebar-link">R-C3D</a></li></ul></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#ru-he-ping-gu" class="sidebar-link">如何评估</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#iou-intersection-over-union" class="sidebar-link">IoU (Intersection over Union)</a></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#ap-average-precision" class="sidebar-link">AP (Average Precision）</a></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#map-mean-average-precision" class="sidebar-link">mAP (mean Average Precision）</a></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#yun-suan-li-zhi-biao" class="sidebar-link">运算力指标</a></li></ul></li><li class="sidebar-sub-header"><a href="/Algorithm/chapter01-two-stage-detection.html#sota-dai-ma-ku" class="sidebar-link">SOTA 代码库</a></li></ul></li><li><a href="/Algorithm/chapter02-yolov1-v5.html" class="sidebar-link">YOLO - You Only Look Once</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="mu-biao-jian-ce---rcnn-xi-lie-qian-tan"><a href="#mu-biao-jian-ce---rcnn-xi-lie-qian-tan" class="header-anchor">#</a> 目标检测 - RCNN 系列浅谈</h1> <h2 id="fa-zhan-gui-ji"><a href="#fa-zhan-gui-ji" class="header-anchor">#</a> 发展轨迹</h2> <center class="half"><img src="https://github.com/hoya012/deep_learning_object_detection/raw/master/assets/deep_learning_object_detection_history.PNG" style="zoom:30%;"><img src="https://static001.infoq.cn/resource/image/ce/e7/ce6f12a5f579217e905851164d0fb4e7.jpg" style="zoom:30%;"></center> <p>目标检测，解决图片中物体&quot;是什么&quot;和&quot;在哪里&quot;的问题。2013 年 Ross Girshick 提出了 RCNN ，随后涌现出很多里程碑的目标检测网络（包括 20 年的 EfficientDet 和 Yolov4 ）。从网络设计思想说，目标检测网络可以分为 two-stage （i.e., RCNN, R-FCN）、one-stage（i.e., Yolo）、anchor-based、anchor-free（i.e., FCOS，CenterNet）。</p> <h2 id="rcnn-she-ji-si-xiang"><a href="#rcnn-she-ji-si-xiang" class="header-anchor">#</a> RCNN 设计思想</h2> <p>双阶段检测，先进行区域生成，生成的区域称为 region proposal，表示该预选框里有可能存在待检测物体，再通过头部网络对预选框做分类和定位。</p> <p>任务流程：图片特征提取 -》 生成 region proposal -》 分类/定位回归</p> <h3 id="r-cnn"><a href="#r-cnn" class="header-anchor">#</a> R-CNN</h3> <p>全名 Regions with CNN Features，是 RCNN 系列的开山之作。结合深度学习和传统计算机视觉。使用 selective search （选择性搜索）提取 region proposal （候选区域），大概 2000 个，使用 SVM 实现分类。</p> <img src="https://pic2.zhimg.com/80/v2-ec320f9a52d0d5630be4a8fc9ea93c61_1440w.jpg" style="zoom:40%;"> <p><strong>存在的不足</strong></p> <ul><li>selective search 耗时。计算子区域的相似性 （颜色、纹理、大小等），合并相似的子区域，迭代，一帧图片耗费 2 秒。</li> <li>重复计算。对每个 region proposal 都要经过 CNN 作特征提取，计算完所有的 region 需要花费 47 秒。</li> <li>模块非端到端训练。提取、分类、回归模块需要分别训练。</li></ul> <h3 id="fast-r-cnn"><a href="#fast-r-cnn" class="header-anchor">#</a> Fast R-CNN</h3> <img src="https://pic4.zhimg.com/80/v2-597bf75a922c054ca038fe4c2fc9655f_1440w.jpg" style="zoom:40%;"> <p><strong>特点</strong></p> <ul><li>加入 RoI Pooling ，采用一个神经网络对全图提取特征</li> <li>但依旧采用 selective search</li></ul> <h3 id="faster-r-cnn"><a href="#faster-r-cnn" class="header-anchor">#</a> Faster R-CNN</h3> <p>Ross 大佬 在 2016 年提出的新模型 ，在 Fast RCNN 基础上，将将特征提取、 region proposal 提取、预测框回归和分类整合到一个网络中，真正实现了端到端的目标检测模型。</p> <img src="https://pic4.zhimg.com/80/v2-35ce8d4e9f9d2c8493f6ca2f894c508f_1440w.jpg" style="zoom:50%;"> <p><strong>整体流程</strong></p> <ul><li>使用 ConvNet （卷积 + 激活 + 池化）对整张图片作特征提取，生成的特征图用于之后的 RPN 和全连接层。</li> <li>RPN （Region Proposal Networks）网络生成候选区域。为 RCNN 子网提供正样本 （前景，框内有物体）和负样本（背景，框内没有物体）。
使用 RoI Pooling 归一候选区域的大小。使用非极大值抑制 NMS 消除高重合度的边框。</li> <li>RCNN 网络，对 RPN 生成的候选区域作多标签分类和边框回归。</li></ul> <p><strong>anchor</strong>
锚点指的是预先设置好的一组检测框，为最后一层特征图的每个稠密网格生成 anchor。通过 anchor 引入多尺度方法，论文中设置了 3 种形状 3 中尺寸的锚，一组锚点有 9 个检测框。</p> <img src="https://pic2.zhimg.com/80/v2-4b15828dfee19be726835b671748cc4d_1440w.jpg" style="zoom:50%;"> <p><strong>正负样本定义</strong>
通过计算 anchor 和 ground truth 的 IoU。论文中,与所有 ground truth 的 IoU 都小于 0.3 为负样本，与任一 ground truth 的 IoU 大于 0.7 为正样本。 正负样本比例 1：3。</p> <p><strong>损失函数</strong></p> <ul><li>RPN： 二分类（anchor box 里是否有物体） +  坐标回归</li> <li>RCNN： 多分类 + 坐标回归</li></ul> <p>最后用多任务学习将两个子网的损失联合起来。</p> <p>$$
\mathcal{L} _\text{RCNN} &amp;= \mathcal{L} _\text{cls} + \mathcal{L} _\text{box}
$$</p> <p>$$
\mathcal{L}({p_i}, {t_i}) &amp;= \frac{1}{N_\text{cls}} \sum_i \mathcal{L}_\text{cls} (p_i, p^<em>_i) + \frac{\lambda}{N _\text{box}} \sum_i p^</em>_i \cdot L_1^\text{smooth}(t_i - t^*_i)
$$</p> <p>同理可得$\mathcal{L} _\text{RPN}$</p> <p>最终$\mathcal{L} &amp;= \mathcal{L} _\text{RPN} + \mathcal{L} _\text{RCNN}$</p> <p><strong>特点</strong></p> <ul><li>实现特征共享。RPN 和分类器共享一套特征图。</li> <li>模型性能很大程度依赖 anchor 的参数，参数的预先设置比较困难。</li> <li>双阶段设计准确率较高，但运算速度慢。所以后来 Ross 大佬提出了以 yolo 为代表的 one-stage 检测网络，将 RPN 和分类器两个子网合并在一起。one-stage 发展至今实现实时推理，并且精度不亚于双阶段检测器。</li></ul> <img src="https://img-blog.csdnimg.cn/20201120192837724.png#pic_center" style="zoom:80%;"> <h2 id="yi-xie-gai-jin-wang-luo-de-xiao-ji-qiao"><a href="#yi-xie-gai-jin-wang-luo-de-xiao-ji-qiao" class="header-anchor">#</a> 一些改进网络的小技巧</h2> <h3 id="fpn"><a href="#fpn" class="header-anchor">#</a> FPN</h3> <p>FPN （Feature Pyramid Network）本身是一个特征提取器，对目标检测器的性能有较大的提升，尤其是检测小物体的能力。网络包含两部：第一部分是自底向上的过程，第二部分是自顶向下和侧向链接。</p> <p><strong>自底向上</strong>
与普通 CNN 相似，每一个卷积块对应特征金字塔的一个 level ，每个卷积块最后一层特征作为 FPN 相应 level 的特征。以 ResNet 为例，这几个特征层相对于原图的步长分别为 4、8、16、32。</p> <p><strong>自顶向下和侧向链接</strong>
自顶向下通过上采样的方式将特征图放大，和上一层特征图尺寸一致。对上一层特征图作 1 x 1 卷积，修正通道数。</p> <p>侧向链接对两层特征图作 pixel-wise 加法，利用了深度卷积网络各个层次特征，保留了高层的语义特征（利于分类）和底层的高分辨率信息（利于定位）。</p> <img src="http://www.tensorinfinity.com/upload/mdfiles/20190325041713_36264.jpg" style="zoom:60%;"> <p><strong>FPN 上的一些改进</strong></p> <ul><li>PANet 先自底向上连，再自顶向下连回去</li> <li>NAS-FPN 使用强化学习方法，将任意两个输入特征层融合到一个输出特征层中</li> <li>2020 年屠榜的 EfficientDet 提出了 BiFPN 结构，在 PAnet 的基础上删除了一些链接</li></ul> <img src="https://pic4.zhimg.com/80/v2-225cc89e2308de82aa2267a9a944762f_1440w.jpg" style="zoom:60%;"> <h3 id="focal-loss"><a href="#focal-loss" class="header-anchor">#</a> Focal Loss</h3> <p>RetinaNet 提出的损失函数，为单阶段检测器设计，解决背景/前景样本不平衡的问题，现在也应用在其他类别不均衡的任务中。</p> <p>在交叉熵的基础上添加惩罚系数，使预测概率越低（也就是越难预测）的样本，计算损失的时候所占比例越大。</p> <p>::: 代码实现
focal loss 代码实现可见 <a href="https://code.devops.xiaohongshu.com/ut/ut-caesar-server/-/blob/release/ut-caesar/utils/loss_utils.py" target="_blank" rel="noopener noreferrer">ut-caesar-codebase<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
:::</p> <h2 id="yi-xie-ji-yu-rcnn-de-yan-sheng"><a href="#yi-xie-ji-yu-rcnn-de-yan-sheng" class="header-anchor">#</a> 一些基于 RCNN 的衍生</h2> <h3 id="r-c3d"><a href="#r-c3d" class="header-anchor">#</a> R-C3D</h3> <p>将目标检测思想运用到视频分段的任务中，使用 3D 卷积作为提取时序特征的 backbone，使用 3D RoI Pooling 归一时序特征。</p> <img src="https://img-blog.csdnimg.cn/20200706095625749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RsNjQzMDUz,size_16,color_FFFFFF,t_70" style="zoom:60%;"> <h2 id="ru-he-ping-gu"><a href="#ru-he-ping-gu" class="header-anchor">#</a> 如何评估</h2> <h3 id="iou-intersection-over-union"><a href="#iou-intersection-over-union" class="header-anchor">#</a> IoU (Intersection over Union)</h3> <h3 id="ap-average-precision"><a href="#ap-average-precision" class="header-anchor">#</a> AP (Average Precision）</h3> <ul><li>对单个类别来说，通过预测框和 ground truth 的 IoU ，判断预测框为 TP 还是 FP。</li> <li>通过调整置信度的阈值，只考虑置信度大于阈值的检测框。对于不同的阈值，我们得到多组 precision 和 recall 值，绘制 PR 曲线。</li> <li>每个“峰值点”往左画一条线段直到与上一个峰值点的垂直线相交。这样画出来的红色线段与坐标轴围起来的曲线下面积就是这个类别的 AP 值。</li></ul> <img src="https://user-images.githubusercontent.com/15831541/43008995-64dd53ce-8c34-11e8-8a2c-4567b1311910.png" style="zoom:60%;"> <h3 id="map-mean-average-precision"><a href="#map-mean-average-precision" class="header-anchor">#</a> mAP (mean Average Precision）</h3> <ul><li>AP 衡量的是单个类别的检测好坏，mAP 就是把所有类别的 AP 取均值。 mAP 是衡量检测器在所有类别上性能的重要指标。</li></ul> <h3 id="yun-suan-li-zhi-biao"><a href="#yun-suan-li-zhi-biao" class="header-anchor">#</a> 运算力指标</h3> <ul><li>FLOPs： floating point operations , 浮点数运算，指计算量，越小越好。</li> <li>FPS：frams per second， 每秒处理帧数，评价执行速度，越大越好</li></ul> <h2 id="sota-dai-ma-ku"><a href="#sota-dai-ma-ku" class="header-anchor">#</a> SOTA 代码库</h2> <ul><li><p><a href="https://www.paperswithcode.com/sota/object-detection-on-coco" target="_blank" rel="noopener noreferrer">Papers with Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 提供了目标检测领域上的论文和公开代码链接，及其在各大 benchmark 的排名。模型性能一目了然。截止至 2021 年 2 月，在 COCO test-dev 上， yolov4 和 efficientdet 占据榜单前列的多个席位。</p></li> <li><p><a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener noreferrer">Detectron2<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 由 FAIR 开源，基于 pytorch。 他的前身 Detectron 是基于 caffe 的第一个比较全面的目标检测工具箱。Detectron2 不仅支持目标检测，还包括实例/语义分割、姿态识别等任务。</p></li> <li><p><a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener noreferrer">mmDetection<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 由商汤和港中文开源，基于 pytorch ，是目前开源检测框架中包含论文数最多的工具箱。作者声称相较于 Detectron2 优点在于性能稍高、训练速度稍快、所需显存稍小。</p></li></ul></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/Algorithm/" class="prev router-link-active">
        简介
      </a></span> <span class="next"><a href="/Algorithm/chapter02-yolov1-v5.html">
        YOLO - You Only Look Once
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"><!----></div></div>
    <script src="/assets/js/app.32056bf2.js" defer></script><script src="/assets/js/2.3d662a95.js" defer></script><script src="/assets/js/8.54d04af0.js" defer></script>
  </body>
</html>
