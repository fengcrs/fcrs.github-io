(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{369:function(t,s,o){"use strict";o.r(s);var a=o(25),_=Object(a.a)({},(function(){var t=this,s=t.$createElement,o=t._self._c||s;return o("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[o("h1",{attrs:{id:"yolo---you-only-look-once"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#yolo---you-only-look-once"}},[t._v("#")]),t._v(" YOLO - You Only Look Once")]),t._v(" "),o("h2",{attrs:{id:"ren-wu-gai-shu"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#ren-wu-gai-shu"}},[t._v("#")]),t._v(" 任务概述")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://pic2.zhimg.com/80/v2-68048e7c5272aba98d7fbc96758ad38d_1440w.jpg",alt:""}}),t._v("\n图像理解的三个任务：分类、检测、分割。")]),t._v(" "),o("p",[t._v("从理解的层次上说：分类 -> image-level、检测 -> region-level、分割 -> pixel-level。")]),t._v(" "),o("p",[t._v("作为图像理解的中层次，目标检测解决的问题是图片中【有什么】、【在哪里】。【有什么】对应着分类，【在哪里】对应着定位。")]),t._v(" "),o("ul",[o("li",[o("p",[t._v("传统目标检测")]),t._v(" "),o("p",[t._v("将检测问题简化成图像分类问题，原理是通过滑动窗口或者选择性搜索，将输入图像切成不同位置不同大小的区域，再对这些区域做分类，从而实现目标检测。\n痛点：慢！且检测不同物体时，需要的检测框的大小和比例都不同。\n"),o("img",{attrs:{src:"https://img2018.cnblogs.com/blog/1423648/201903/1423648-20190317100459585-1587811888.png",alt:""}})])]),t._v(" "),o("li",[o("p",[t._v("双阶段检测")]),t._v(" "),o("p",[t._v("RCNN 系列的提出，提高了目标检测的精度和速度。但还是需要中间区域 （Region Proposal）来判断候选框里有没有物体，然后再做目标的分类和边框回归。\n"),o("img",{attrs:{src:"https://pic4.zhimg.com/80/v2-597bf75a922c054ca038fe4c2fc9655f_1440w.jpg",alt:""}})])])]),t._v(" "),o("p",[t._v("这个时候！YOLO 横空出世！提出了目标检测的另一种思路，将目标检测转换成一个回归问题，在一次前馈中同时推理出有没有物体、有什么物体、以及检测框的位置。2016 年，yolo1 就取得了 45 fps 的推理速度，且在当时其他实时的目标检测算法中是精度最高的。发展至今 yolo5 已经成为实时目标检测的首选框架。")]),t._v(" "),o("h2",{attrs:{id:"yolo-she-ji-li-nian"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#yolo-she-ji-li-nian"}},[t._v("#")]),t._v(" yolo 设计理念")]),t._v(" "),o("h3",{attrs:{id:"wang-luo-jie-gou"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#wang-luo-jie-gou"}},[t._v("#")]),t._v(" 网络结构")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://pic4.zhimg.com/80/v2-5d099287b1237fa975b1c19bacdfc07f_1440w.jpg",alt:""}}),t._v("\nyolo 的网络结构非常简单，就是单纯的卷积 + 池化 + 全链接，和分类网络几乎无差异。网络的输出为 7x7x30 的张量，包含目标检测中分类和定位的所有信息。一次前馈可以得到所有信息，所以取名 you only look once.")]),t._v(" "),o("p",[t._v("理解 yolo，核心就是理解网络的"),o("strong",[t._v("输出构成")]),t._v("和"),o("strong",[t._v("损失函数")]),t._v("。")]),t._v(" "),o("h2",{attrs:{id:"wang-luo-shu-chu"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#wang-luo-shu-chu"}},[t._v("#")]),t._v(" 网络输出")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://pic2.zhimg.com/v2-850969b88edb6f9ce6506ab241333aae_1440w.jpg?source=172ae18b",alt:""}})]),t._v(" "),o("ul",[o("li",[o("p",[t._v("7x7 网格")]),t._v(" "),o("p",[t._v("输入大小为 448x448 的图像，通过卷积网络降采样 64 倍，得到 7x7 的特征图。")])]),t._v(" "),o("li",[o("p",[t._v("30 维向量")]),t._v(" "),o("p",[t._v("每个网格对应一个 30 维向量，其中包含了 20 个检测对象的分类概率、每个网格为中心负责预测的 2 个 bbox 的置信度、以及这两个 bbox 的位置表示。")]),t._v(" "),o("ul",[o("li",[o("p",[t._v("分类概率： YOLO 支持识别 20 种不同的对象（人、鸟、猫、汽车、椅子等），所以这里有 20 个值表示该网格位置存在任一种对象的概率。")])]),t._v(" "),o("li",[o("p",[t._v("bbox 位置： 4 个数值的坐标表示 $(\\text {center_x}, \\text {center_y}, \\text {width}, \\text {height})$")])]),t._v(" "),o("li",[o("p",[t._v("bbox 置信度： $\\text { Confidence } = \\text { Pr }(\\text { Object }) * \\text { IOU }_\\text { pred }^\\text { truth }$，然后看 2 个 bbox 的 IOU，哪个比较大（更接近 ground truth ），就由哪个 bbox 来负责预测该对象是否存在.")])])]),t._v(" "),o("p",[t._v("30 维向量 = 20 个对象的分类概率 + 2 个 bbox * 4 个坐标 + 2 个 bbox 的置信度")])])]),t._v(" "),o("p",[t._v("所以总体网络输出为 7x7x30 的张量")]),t._v(" "),o("h3",{attrs:{id:"sun-shi-han-shu"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#sun-shi-han-shu"}},[t._v("#")]),t._v(" 损失函数")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://sns-img-qc.xhscdn.com/20210526135958222-v2-d3de934214afa4a5790463f7f23f2e38_1440w.jpeg",alt:""}}),t._v("\n损失函数中记录了三种误差信息：边框、置信度、分类，损失函数中各个损失项与输出的 30 维向量中的内容对应。细节见上图。")]),t._v(" "),o("p",[t._v("对于损失函数中权重的设置，")]),t._v(" "),o("ul",[o("li",[t._v("更重视 8 维的坐标预测，给这些损失前面赋予更大的权重 5。")]),t._v(" "),o("li",[t._v("对没有 object 的 box 的 confidence loss，赋予小的权重 0.5。")]),t._v(" "),o("li",[t._v("有 object 的 box 的 confidence loss 和类别的 loss ，权重正常取 1。")])]),t._v(" "),o("p",[t._v("总的来说，就是用网络输出与样本标签的各项内容的误差平方和作为一个样本的整体误差。 损失函数中的几个项是与输出的 30 维向量中的内容相对应的。")]),t._v(" "),o("h3",{attrs:{id:"gong-zuo-liu-cheng"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#gong-zuo-liu-cheng"}},[t._v("#")]),t._v(" 工作流程")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://pic4.zhimg.com/80/v2-40c8cbed60aba0fe2faa38e240b8563b_1440w.jpg",alt:""}})]),t._v(" "),o("p",[o("img",{attrs:{src:"https://pic3.zhimg.com/80/v2-258df167ee37b5594c72562b4ae61d1a_1440w.jpg",alt:""}})]),t._v(" "),o("p",[t._v("训练好的 YOLO 网络，输入一张图片，将输出一个 7x7x30 的张量来表示图片中所有网格包含的对象（概率）、该对象可能的 2 个位置（ bbox ）和置信度。\n采用 NMS（Non-maximal suppression，非极大值抑制）算法，选择得分最高的作为输出，去掉与该输出重叠度高的框。")]),t._v(" "),o("p",[t._v("yolo1 使用 softmax 做类别分类，所以一张图片最多可以检测出 7 x 7 = 49 个对象。在检测密集物体、小物体时效果不理想。那这个问题怎么解决呢？请往下看。")]),t._v(" "),o("h2",{attrs:{id:"yolo-fa-zhan"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#yolo-fa-zhan"}},[t._v("#")]),t._v(" yolo 发展")]),t._v(" "),o("p",[t._v("从 yolo2 到 5，每一个版本改进都是在 yolo 的中心设计思想的基础上引入当时 cv 领域的优秀 trick 和网络结构的调整，每篇论文的可读性都非常高，这里只挑选介绍。")]),t._v(" "),o("h3",{attrs:{id:"xian-yan-kuang"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#xian-yan-kuang"}},[t._v("#")]),t._v(" 先验框")]),t._v(" "),o("div",{staticClass:"custom-block tip"},[o("p",{staticClass:"custom-block-title"},[t._v("改进前后对比")]),t._v(" "),o("p",[t._v("改进前： 一个网格预测两个检测框，边框大小全靠网络学习")]),t._v(" "),o("p",[t._v("改进后： 预设检测框更符合数据集分布")])]),t._v(" "),o("p",[t._v("yolo2 中，作者借鉴 Faster R-CNN 的 9 个预设矩形框，也引入 anchor box 的概念。不同的是，yolo2 中的 Anchor Box 的宽高不经过人为获得，而是将训练数据集中的 ground truth 矩形框全部拿出来，用 k-means 聚类得到先验框的宽和高。先验框只与检测框的 w、h 有关，与 x、y 无关。")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://sns-img-qc.xhscdn.com/20210525183459786-wecom20210525-1834172x.png",alt:""}})]),t._v(" "),o("p",[t._v("例如 yolo2 使用 5 个 Anchor Box，那么 k-means 聚类的类别中心个数设置为 5。加入了聚类操作之后，引入 Anchor Box 之后，mAP 上升。")]),t._v(" "),o("p",[t._v("k-means 对训练集中的边界框做了聚类分析，定义聚类点之间的距离函数如下：")]),t._v(" "),o("p",[t._v("$$\nd(\\text { box }, \\text { centroid })=1-\\text { IOU(box, centroid) }\n$$")]),t._v(" "),o("h3",{attrs:{id:"bian-kuang-yue-shu"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#bian-kuang-yue-shu"}},[t._v("#")]),t._v(" 边框约束")]),t._v(" "),o("div",{staticClass:"custom-block tip"},[o("p",{staticClass:"custom-block-title"},[t._v("改进前后对比")]),t._v(" "),o("p",[t._v("改进前： 预测边框坐标和大小的绝对值，可能造成差值过大模型难以收敛")]),t._v(" "),o("p",[t._v("改进后： 边框约束，更稳定")])]),t._v(" "),o("p",[t._v("同样借鉴 RCNN 系列，网络输出的 bbox 的位置坐标，其实是偏移量 tx, ty 和尺度缩放 tw, th，想要得到检测框的真是坐标，需要通过图中公式解码。")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://sns-img-qc.xhscdn.com/20210526104503784-wecom20210525-2032422x.png",alt:""}})]),t._v(" "),o("p",[t._v("$\\sigma\\left(t_{x}\\right)$，$\\sigma\\left(t_{y}\\right)$ 是基于矩形框中心点左上角格点坐标的偏移量，$p_{w}$，$p_{h}$ 是聚类得到的先验框宽和高。")]),t._v(" "),o("p",[t._v("使用 "),o("strong",[t._v("sigmoid")]),t._v(" 作为激活函数，约束偏移量在 0 到 1 之间，保证预测的中心坐标在该网格内。")]),t._v(" "),o("h3",{attrs:{id:"duo-chi-du-yu-ce"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#duo-chi-du-yu-ce"}},[t._v("#")]),t._v(" 多尺度预测")]),t._v(" "),o("div",{staticClass:"custom-block tip"},[o("p",{staticClass:"custom-block-title"},[t._v("改进前后对比")]),t._v(" "),o("p",[t._v("改进前： 只在 7x7 的特征图上预测，容易缺失信息")]),t._v(" "),o("p",[t._v("改进后： 三种不同大小的特征图，负责检测不同大小的物体")])]),t._v(" "),o("p",[t._v("yolo3 引入多尺度预测，为了更好的预测不同大小的目标物体。")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://sns-img-qc.xhscdn.com/20210525202108400-wecom20210525-2020252x.png",alt:""}})]),t._v(" "),o("p",[t._v("输入 416 x 416 尺寸的图片，通过卷积分别实现降采样 8、16、32 倍，得到三个尺寸的特征图。特征图上每个网格单元预测 3 个 box，每个 box 需要有 (x, y, w, h, confidence) 五个基本参数，然后还要有 80 个类别的概率。所以 255 由来就是 （5 + 80）* 3。")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://sns-img-qc.xhscdn.com/20210525202206478-wecom20210525-2021402x.png",alt:""}})]),t._v(" "),o("p",[t._v("对于不同尺寸的特征图，特征图越小，经过的卷积层数越多，在原图上的感受野就越大。所以特征图越小，越适合预测大尺寸的物体，使用的先验框就越大。")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://sns-img-qc.xhscdn.com/20210525165802733-wecom20210525-1657302x.png",alt:""}})]),t._v(" "),o("p",[t._v("模型上的改进也很大，模型 backbone 使用 darknet，同时借鉴了残差结构和特征金字塔的张量 concat 等，不多介绍。")]),t._v(" "),o("p",[t._v("损失函数新定义")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://www.zhihu.com/equation?tex=loss_%7BN_1%7D+%3D+%5Clambda_%7Bbox%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7D%5B%28t_x+-+t_x%27%29%5E2+%2B+%28t_y+-+t_y%27%29%5E2%5D%7D+%5C%5C+%2B%5Clambda_%7Bbox%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7D%5B%28t_w+-+t_w%27%29%5E2+%2B+%28t_h+-+t_h%27%29%5E2%5D%7D++%5C%5C+-+%5Clambda_%7Bobj%7D%5Csum_%7Bi%3D0%7D%5E%7BN%5Ctimes+N%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7Dlog%28c_%7Bij%7D%29%7D+%5C%5C+-%5Clambda_%7Bnoobj%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bnoobj%7Dlog%281-c_%7Bij%7D%29%7D+%5C%5C+-%7B%5Clambda%7D_%7Bclass%7D%5Csum_%7Bi%3D0%7D%5E%7BN_1%5Ctimes+N_1%7D%5C%5C%5Csum_%7Bj%3D0%7D%5E%7B3%7D%7B1_%7Bij%7D%5E%7Bobj%7D+%5Csum_%7Bc+%5Cin+classes+%7D%5Bp_%7Bij%7D%27%28c%29log%28p_%7Bij%7D%28c%29%29%2B%281-p_%7Bij%7D%27%28c%29%29log%281-p_%7Bij%7D%28c%29%29%5D+%7D+",alt:""}})]),t._v(" "),o("p",[t._v("softmax + mse 改为 sigmoid + bce，支持多标签分类，解决了同一网格中目标可能有重叠的类别标签。")]),t._v(" "),o("p",[t._v("总 loss 为三个尺寸的特征图上 loss 之和")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://www.zhihu.com/equation?tex=Loss%3Dloss_%7BN_1%7D++%2Bloss_%7BN_2%7D+%2Bloss_%7BN_3%7D++%5C%5C",alt:""}})]),t._v(" "),o("p",[t._v("引入多尺度特征图后，输入为 416 x 416 的图片能生成的预测框总数量为 52 * 52 * 3 + 26 * 26 * 3 + 13 * 13 * 3 = 10647，相比 yolo1 中 7 * 7 * 2 个检测框新增很多，大大提升了检测密集物体的能力。")]),t._v(" "),o("h2",{attrs:{id:"xing-neng-yi-lan"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#xing-neng-yi-lan"}},[t._v("#")]),t._v(" 性能一览")]),t._v(" "),o("p",[o("img",{attrs:{src:"https://sns-img-qc.xhscdn.com/20210526102235038-wecom20210525-2031352x.png",alt:""}}),t._v("\nyolo4 的性能总结：快，也准！v5 虽然没有官方的性能评估，但根据 "),o("a",{attrs:{href:"https://github.com/ultralytics/yolov5",target:"_blank",rel:"noopener noreferrer"}},[t._v("GitHub"),o("OutboundLink")],1),t._v(" ，yolo5 在 tesla V100 上最高可达 250 FPS，且保持了较高的 mAP。而且 yolov5 训练所需时间相比 v4 更少。")]),t._v(" "),o("p",[t._v("yolo4 和 5 都支持不同大小的 backbone 模型，可根据使用场景选择合适的模型部署。")]),t._v(" "),o("h2",{attrs:{id:"reference"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#reference"}},[t._v("#")]),t._v(" Reference")]),t._v(" "),o("ol",[o("li",[t._v("You Only Look Once: Unified, Real-Time Object Detection https://arxiv.org/abs/1506.02640")]),t._v(" "),o("li",[t._v("YOLO9000: Better, Faster, Stronger https://arxiv.org/abs/1612.08242")]),t._v(" "),o("li",[t._v("YOLOv3: An Incremental Improvement https://arxiv.org/abs/1804.02767")]),t._v(" "),o("li",[t._v("YOLOv4: Optimal Speed and Accuracy of Object Detection https://arxiv.org/abs/2004.10934")]),t._v(" "),o("li",[t._v("YOLO v5 GitHub https://github.com/ultralytics/yolov5")])]),t._v(" "),o("p",[t._v("一些彩蛋")]),t._v(" "),o("blockquote",[o("p",[o("em",[t._v("I have a lot of hope that most of the people using computer vision are just doing happy, good stuff with it, like counting the number of zebras in a national park, or tracking their cat as it wanders around their house. But computer vision is already being put to questionable use and as researchers we have a responsibility to at least consider the harm our work might be doing and think of ways to mitigate it. We owe the world that much.")])])]),t._v(" "),o("p",[t._v("（论文尾部，作者对于行业的思考）")]),t._v(" "),o("blockquote",[o("p",[t._v("yolo3 论文短小的风格、诡异的文风、神奇的措辞，彻底放飞自我：我随便写的，你们也就随便看看吧，反正我的效果好你肯定要引用我，还顺带嘲讽了一波 retinanet。")])]),t._v(" "),o("p",[t._v("（以上立场来自知乎某网友，本人概不负责）")]),t._v(" "),o("p",[t._v("最后附上 yolo 一作的彩虹小白马简历\n"),o("img",{attrs:{src:"https://sns-img-qc.xhscdn.com/20210526115202887-wecom20210526-1151422x.png",alt:""}})])])}),[],!1,null,null,null);s.default=_.exports}}]);